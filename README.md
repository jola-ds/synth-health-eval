# Synthetic Data Augmentation for Geriatric Health in Nigeria

![Python](https://img.shields.io/badge/python-3.8%2B-blue) ![License](https://img.shields.io/badge/license-MIT-green)

**Can synthetic data solve the "Small Data" problem in healthcare?**
This project evaluates the utility, fidelity, and privacy of synthetic data
generated from a small (N=134) real-world clinical dataset of geriatric patients
in Ibadan, Nigeria.

## Executive Summary

**Synthetic Data Augmentation** offers a transformative solution to the dual
challenges of **Data Privacy** and **Data Scarcity** in healthcare. This project
applies globally validated synthetic data frameworks to a specific, high-risk
context: **Geriatric Hypertension Prediction in Nigeria**.

### Global Context

During the COVID-19 pandemic, the **National COVID Cohort Collaborative (N3C)**
proved that synthetic data is not just a theoretical conceptâ€”it is a lifesaver.
By generating "digital twins" of patient cohorts, researchers bypassed strict
privacy silos (GDPR/HIPAA) to accelerate pandemic response, enabling rapid
cross-institutional research without exposing sensitive patient records.

### ðŸ‡³ðŸ‡¬ Local Need

Sub-Saharan Africa bears 25% of the global disease burden but contributes
minimal data to global AI models, leading to algorithmic bias. In geriatric
healthcare, datasets are often fragmented, paper-based, and too small for modern
AI. This project addresses the **"Cold Start" problem** for African AI,
demonstrating how we can ethically augment a small dataset ($N=134$) into a
robust resource for hypertension prediction without compromising patient privacy
or sovereignty.

This project demonstrates how synthetic generation serves as an **Equity
Tool**, allowing Nigerian researchers to build robust, locally relevant AI
models despite infrastructural constraints.

> **Note:** *For a full picture of the synthetic data landscape in healthcare,
read the full [Literature Review & Background](0_domain_study/README.md)*

---

## Analysis Approach

Our methodology followed a strict four-phase pipeline:

1. **Imputation Shootout:** We compared MICE, KNN, and MissForest to handle
missing values. **MICE** won with the lowest distribution shift.

2. **Generator Screening:** We trained and compared Gaussian Copula
(Statistical) and CTGAN (Deep Learning).

3. **The "Master Loop":** We ran a **Repeated Stratified K-Fold
Cross-Validation (5x5)** to evaluate models under four scenarios:
* *Scenario A (Baseline):* Train Real, Test Real.
* *Scenario B (Fidelity):* Train Synthetic, Test Real.
* *Scenario C (Scale):* Train Large Synthetic, Test Real.
* *Scenario D (Augment):* Train Real + Synthetic, Test Real.

4. **Fidelity Audit:** We used KS Tests, Correlation Matrices, and Adversarial
AUC to quantify the statistical quality of the synthetic data.

> **Note:** *For a full picture of our methodology , read the full [Analysis
Approach](2_data_analysis/methodology.md).*

---

## Research Conclusions

**1. Augmentation Works (The "Sweet Spot"):**
Augmenting the small real dataset with **50% synthetic data** generated by a
**Gaussian Copula** model maintained the baseline F1-Score (**0.825** vs 0.826)
while potentially increasing model robustness. This was the optimal strategy.

**2. Statistical Models Beat Deep Learning:**
For this small sample size, the statistical **Gaussian Copula** generator
significantly outperformed the deep learning-based **CTGAN**.

* **Copula:** Captured ~88% of the signal (Fidelity F1: 0.727).
* **CTGAN:** Failed to converge, performing near random chance (Fidelity F1:
0.470).

**3. Privacy is Preserved:**
Both generators successfully passed the Distance to Closest Record (DCR)
privacy audit, ensuring no real patient records were memorized or leaked.

> **Note:** *For a better picture of our findings, read the full [Analysis
Report](2_data_analysis/analysis_report.md).*

---

## Confidence & Limitations

### Confidence Level: High

* **Robustness:** Results are based on **25 independent runs** (5 folds x 5
repeats), minimizing random chance.
* **Consistency:** The "Inverted U" curve in augmentation performance was
consistent across multiple tests.
* **Validation:** Fidelity metrics (KS=0.92, Correlation=0.89) strongly support
the utility findings.

### Limitations

* **Sample Size:** The original dataset is very small (N=134). While this makes
the augmentation result impressive, it limits the complexity of models we can
train.
* **Linearity:** The Gaussian Copula assumes somewhat linear relationships
(Gaussian correlations). It may miss complex, non-linear interactions that a
converged GAN could capture if more data were available.
* **Generalizability:** Findings are specific to this geriatric hypertension
dataset and may not transfer 1:1 to other domains (e.g., imaging).

---

## ðŸš€ Future Research

1. **Bayesian Networks:** Manually defining causal edges (e.g., `Age ->
Hypertension`) could improve fidelity beyond the Copula by enforcing known
biological constraints.
2. **Differential Privacy:** Implementing formal epsilon-differential privacy
during training to provide mathematical privacy guarantees.
3. **External Validation:** Testing the "Augmented" model on a completely
external dataset from a different clinic to prove true generalization.

---

## ðŸ“‚ Repository Structure

* **`1_datasets/`**: Raw and imputed data, plus the Data Dictionary.
* **`2_data_analysis/`**: The core findings.
* `analysis_report.md`: Detailed performance tables and graphs.
* `methodology.md`: Full technical explanation of the pipeline.
* `results/`: Raw CSVs from the evaluation loop.
* **`3_notebooks/`**: Interactive Jupyter notebooks for exploration.
* `01_preprocessing.ipynb`: Data cleaning and imputation.
* `02_screening.ipynb`: Visual comparison of generators.
* `03_evaluation.ipynb`: The Master Loop execution.
* **`4_src/`**: Python source code modules (`generation.py`, `evaluation.py`,
etc.).

---

## ðŸ› ï¸ Getting Started

### Prerequisites

* Python 3.8+
* pip

### Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/yourusername/synth-health-eval.git
    cd synth-health-eval
    ```

2. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

---

## ðŸ’» Usage

### 1. Reproduce the Analysis

Run the notebooks in order to replicate the full pipeline:

1. **Preprocessing:** `jupyter notebook 3_notebooks/01_preprocessing.ipynb`
2. **Screening:** `jupyter notebook 3_notebooks/02_screening.ipynb`
3. **Evaluation:** `jupyter notebook 3_notebooks/03_evaluation.ipynb`

### 2. Generate Synthetic Data

Use the core modules directly:

```python
from src.generation import GeneratorWrapper
import pandas as pd

# Load Data

data = pd.read_csv("1_datasets/imputed_data.csv")

# Train & Sample

gen = GeneratorWrapper(model_type='copula')
gen.fit(data)
synthetic_data = gen.sample(100)
```

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE)
file for details.

---

## âœï¸ Citation

If you use this work, please cite:
> Omotunde, M. (2025). *Synthetic Data Augmentation for Geriatric Health in
Nigeria*.
